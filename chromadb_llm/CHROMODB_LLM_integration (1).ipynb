!pip install chromadb sentence-transformers PyMuPDF
from google.colab import files
uploaded = files.upload()
!pip install PyMuPDF
import fitz  # This is PyMuPDF

def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text
  



pdf_path = next(iter(uploaded))  # Get uploaded file name
full_text = extract_text_from_pdf(pdf_path)
print(full_text[:1000])  # Preview text
def split_into_chunks(text, chunk_size=500):
    words = text.split()
    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

chunks = split_into_chunks(full_text)
print(f"Total Chunks: {len(chunks)}")
from sentence_transformers import SentenceTransformer

retrieval_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = retrieval_model.encode(chunks)


!pip install chromadb

import chromadb

# Use EphemeralClient for in-memory (Colab-friendly)
chroma_client = chromadb.EphemeralClient()

# Create or get collection
collection = chroma_client.get_or_create_collection("pdf_chunks")

import torch
import numpy as np
from sentence_transformers import util

# Ask your question
question = "What are the key skills in the PDF?"

# Embed the question
question_embedding = retrieval_model.encode(question)
question_tensor = torch.tensor(question_embedding, dtype=torch.float32)

# Convert stored embeddings to tensor
embeds_tensor = torch.tensor(np.array(embeddings), dtype=torch.float32)

# Compute cosine similarities
similarities = util.cos_sim(question_tensor, embeds_tensor)

# Get the best match index (convert tensor to int)
best_index = similarities.argmax().item()

# Print the best matching chunk only
print("\nðŸ“š Answer (best matching chunk):\n")
print(chunks[best_index])
chunks[best_index] # Install if not already
!pip install huggingface_hub

# Python Code
from huggingface_hub import InferenceClient

# Replace with your actual Hugging Face token
hf_token = "access token"

# Create inference client for the model
client = InferenceClient("mistralai/Mistral-7B-Instruct-v0.1", token=hf_token)

# Your input
# chunk_text = """
# RAHUL BAMANIYA +91 9340475725 | rahul.bamaniya0907@gmail.com.com | Linkedin | Github |
# OBJECTIVE: Enthusiastic Computer Science student with expertise in AI, ML, and Full-Stack Development.
# Seeking an internship to apply AI/ML and software engineering skills in solving real-world challenges related to space and satellite technologies.
# EDUCATION:
# â€¢ B.E. Computer Engineering - SGSITS, Indore - CGPA: 7.6
# â€¢ XII (MP Board) - 94%
# â€¢ X (MP Board) - 93%
# PROJECTS:
# â€¢ Satellite Image Classification using Deep Learning - TensorFlow, OpenCV, CNN
# """
chunk_text=chunks[best_index]
# Create your prompt
summary_prompt = f"Give the sde related skills from this resume:\n{chunk_text}"

# Generate summary
response = client.text_generation(prompt=summary_prompt, max_new_tokens=200, temperature=0.7)
print("Summary:\n", response) 

# Generate text
response = client.text_generation(prompt=summary_prompt, max_new_tokens=200)

print("Itinerary:\n", response)
